{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS5CiXL1Pwyu"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7pe_oGdrSN1",
        "outputId": "7801a19c-6db8-42e8-b742-9e584cf2f190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc1ptwP5xHwq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "    return version.split('+')[0]\n",
        "\n",
        "def format_cuda_version(version):\n",
        "    return 'cu' + version.replace('.', '')\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkU1ucn9xFtC",
        "outputId": "0ca35c83-8f4a-4310-cb5b-d214852b373b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu113.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.1.0.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (2.1.0.post1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPgvoKnlyEGN",
        "outputId": "1a86f900-ce09-4fbe-812d-d41a3ae309d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch_geometric\n",
        "\n",
        "torch_geometric.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54aTQ2Utyaa5"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4lnyakgLa2X",
        "outputId": "4fc960e5-3750-47d5-e82c-d233da4e29e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (0.41.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.56.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (3.0.9)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->shap) (4.12.0)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "!pip install ftfy\n",
        "!pip install shap\n",
        "!pip install transformers\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbL2GtlELb1n"
      },
      "outputs": [],
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"bpe_simple_vocab_16e6.txt.gz\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"bpe_simple_vocab_16e6.txt.gz\", \"1sazrmZm-bsAyLap-kvFCVn8uXdGppVo-\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"clip_model.pt\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"clip_model.pt\", \"1UO7E2nu_4-C5RRcTtYKqCRjLPUtzV2NQ\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"Sample.zip\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"Sample.zip\", \"1wIUd5ojRMm1SDRHmk0aZPT63shNiK2aD\"]])\n",
        "\n",
        "downloadFiles([[\"harmp_memes_tgt.zip\", \"1K6xfdut1J4DaAZipcyby3MuhgaaJPsGP\"]])\n",
        "downloadFiles([[\"harmc_memes_tgt.zip\", \"1v9Dme-EAJhdsFqDGxl8IeSuYa6sQw7ey\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6GfwrxMLf_S"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "f = zipfile.ZipFile(\"Sample.zip\", \"r\")\n",
        "for file in f.namelist():\n",
        "    f.extract(file, \"./\")\n",
        "f.close()\n",
        "\n",
        "f = zipfile.ZipFile(\"harmp_memes_tgt.zip\", \"r\")\n",
        "for file in f.namelist():\n",
        "    f.extract(file, \"./\")\n",
        "f.close()\n",
        "\n",
        "f = zipfile.ZipFile(\"harmc_memes_tgt.zip\", \"r\")\n",
        "for file in f.namelist():\n",
        "    f.extract(file, \"./\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyZ78xSnLjD0",
        "outputId": "1c79da30-983c-4731-836e-dae594530442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "# from torchnlp import encoders\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import  mean_absolute_error\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
        "from pathlib import Path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir3HfEWfL7Y6"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGjOaPoJMCtL",
        "outputId": "879409fa-6c7d-4481-c797-458eb8098bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(355, 4)\n",
            "(177, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-78c72020-2b4e-4334-b4fe-365f7d9b44b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>image</th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>memes_5145</td>\n",
              "      <td>memes_5145.png</td>\n",
              "      <td>[not harmful]</td>\n",
              "      <td>\"IT'S BEEN THE SAME\\nSTORY EVER SINCE\\nI CAN R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>memes_1189</td>\n",
              "      <td>memes_1189.png</td>\n",
              "      <td>[somewhat harmful, organization]</td>\n",
              "      <td>WE ARE LEADERS\\nR\\nOF THE PARTY OF HATE,\\nDIVI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>memes_8188</td>\n",
              "      <td>memes_8188.png</td>\n",
              "      <td>[not harmful]</td>\n",
              "      <td>WE NEED TO BUILDA WALL\\nph\\nNOT TO KEEP THEM O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>memes_3480</td>\n",
              "      <td>memes_3480.png</td>\n",
              "      <td>[not harmful]</td>\n",
              "      <td>LIBERTARIAN\\n\"A PERSON WHO DOESN'T\\nFOLLOW UP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>memes_4345</td>\n",
              "      <td>memes_4345.png</td>\n",
              "      <td>[not harmful]</td>\n",
              "      <td>THE MODERATORS 12 MINUTES INTO\\nTHE TRUMP-BIDẺ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78c72020-2b4e-4334-b4fe-365f7d9b44b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78c72020-2b4e-4334-b4fe-365f7d9b44b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78c72020-2b4e-4334-b4fe-365f7d9b44b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           id           image                            labels  \\\n",
              "0  memes_5145  memes_5145.png                     [not harmful]   \n",
              "1  memes_1189  memes_1189.png  [somewhat harmful, organization]   \n",
              "2  memes_8188  memes_8188.png                     [not harmful]   \n",
              "3  memes_3480  memes_3480.png                     [not harmful]   \n",
              "4  memes_4345  memes_4345.png                     [not harmful]   \n",
              "\n",
              "                                                text  \n",
              "0  \"IT'S BEEN THE SAME\\nSTORY EVER SINCE\\nI CAN R...  \n",
              "1  WE ARE LEADERS\\nR\\nOF THE PARTY OF HATE,\\nDIVI...  \n",
              "2  WE NEED TO BUILDA WALL\\nph\\nNOT TO KEEP THEM O...  \n",
              "3  LIBERTARIAN\\n\"A PERSON WHO DOESN'T\\nFOLLOW UP ...  \n",
              "4  THE MODERATORS 12 MINUTES INTO\\nTHE TRUMP-BIDẺ...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir_cov = 'Sample/Harm-P/images'\n",
        "train_path_cov = \"Sample/Harm-P/Harm-P-Full/train.jsonl\"\n",
        "dev_path_cov   = \"Sample/Harm-P/Harm-P-Full/val.jsonl\"\n",
        "test_path_cov  = \"Sample/Harm-P/Harm-P-Full/test.jsonl\"\n",
        "\n",
        "#train_path_cov_tar = \"/content/harmc_memes_tgt/target_train.jsonl\"\n",
        "#dev_path_cov_tar   = \"/content/harmc_memes_tgt/target_val.jsonl\"\n",
        "#test_path_cov_tar  = \"/content/harmc_memes_tgt/target_test.jsonl\"\n",
        "\n",
        "\n",
        "train_samples_frame_cov = pd.read_json(train_path_cov, lines=True)\n",
        "\n",
        "test_samples_frame_cov = pd.read_json(test_path_cov, lines=True)\n",
        "print(test_samples_frame_cov.shape)\n",
        "\n",
        "dev_samples_frame_cov = pd.read_json(dev_path_cov, lines=True)\n",
        "print(dev_samples_frame_cov.shape)\n",
        "\n",
        "test_samples_frame_cov.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q-ZW5-NMEHD"
      },
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cysNYFlhMG55"
      },
      "outputs": [],
      "source": [
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFAH2VNzMKUE"
      },
      "outputs": [],
      "source": [
        "## Load the ROI features (Covid)\n",
        "train_ROI_cov = torch.load(\"Sample/features/harmeme_ROI_MOMENTA/cov/memes_harmfulness/harmeme_cov_train_ROI.pt\")\n",
        "val_ROI_cov = torch.load(\"Sample/features/harmeme_ROI_MOMENTA/cov/memes_harmfulness/harmeme_cov_val_ROI.pt\")\n",
        "test_ROI_cov = torch.load(\"Sample/features/harmeme_ROI_MOMENTA/cov/memes_harmfulness/harmeme_cov_test_ROI.pt\")\n",
        "# Load the ENT features\n",
        "train_ENT_cov = torch.load(\"Sample/features/harmeme_ENT_MOMENTA/cov/harmeme_cov_harmfulness/harmeme_cov_train_ent.pt\")\n",
        "val_ENT_cov = torch.load(\"Sample/features/harmeme_ENT_MOMENTA/cov/harmeme_cov_harmfulness/harmeme_cov_val_ent.pt\")\n",
        "test_ENT_cov = torch.load(\"Sample/features/harmeme_ENT_MOMENTA/cov/harmeme_cov_harmfulness/harmeme_cov_test_ent.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr1qRMe8MOSz"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtPEwwlzMSqe"
      },
      "outputs": [],
      "source": [
        "preprocess = Compose([\n",
        "    Resize([100,100]),\n",
        "    ToTensor()\n",
        "    ])\n",
        "tokenizer = SimpleTokenizer()\n",
        "from torchvision import transforms\n",
        "def img_feature_representation(in_img):\n",
        "    image = Image.open(in_img)\n",
        "    image = preprocess((image.convert('RGB')))\n",
        "    image = torch.tensor(np.stack(image)).cuda()\n",
        "    mean, std = image.mean([1,2]), image.std([1,2])\n",
        "    image -= mean[:, None, None]\n",
        "    image /= std[:, None, None]    \n",
        "    return image\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGdSlYkcMUgR"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "fater_RCNN_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLu-6uS7MY4E"
      },
      "outputs": [],
      "source": [
        "def fater_RCNN_process(model,image_FRCNN_input,image):\n",
        "    # get region_feature \n",
        "    model.eval()\n",
        "    region_feature = []\n",
        "    score_threshold = .5\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "      prediction = model([image_FRCNN_input.to(device)])\n",
        "      if len(prediction[0]['scores'])!=0:\n",
        "        max_score= prediction[0]['scores'][0]\n",
        "        max_num=0\n",
        "        for score in prediction[0]['scores']:\n",
        "          i=0\n",
        "          if prediction[0]['scores'][i] > max_score:\n",
        "            max_score = prediction[0]['scores'][i]\n",
        "          i+=1\n",
        "\n",
        "        boxes=prediction[0]['boxes'][prediction[0]['scores'] == max_score]\n",
        "      else: \n",
        "        boxes=prediction[0]['boxes']\n",
        "      # print(boxes) tensor([[  0.0000, 346.7071, 369.1440, 647.5507]], device='cuda:0')\n",
        "  \n",
        "     \n",
        "    # get crop img\n",
        "    im = Image.open(image)\n",
        "   \n",
        "    width, height = im.size\n",
        "    if len(boxes)!=0:\n",
        "      left = float(boxes[0][0].cpu().detach().numpy()) \n",
        "      top = float(boxes[0][1].cpu().detach().numpy())\n",
        "      right = float(boxes[0][2].cpu().detach().numpy())\n",
        "      bottom = float(boxes[0][3].cpu().detach().numpy())\n",
        "      im1 = im.crop((left, top, right, bottom))\n",
        "    else:\n",
        "      im1 = im\n",
        "    resize = torchvision.transforms.Resize([100,100])\n",
        "    im1 = resize(im1)\n",
        "    # print(im1.size) #100\n",
        "    return im1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDFhWfpgy8Ym"
      },
      "outputs": [],
      "source": [
        "def fater_RCNN_process(model,image_FRCNN_input,image):\n",
        "    # get region_feature \n",
        "    model.eval()\n",
        "    region_feature = []\n",
        "    score_threshold = .5\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "      prediction = model([image_FRCNN_input.to(device)])\n",
        "      if len(prediction[0]['scores'])!=0:\n",
        "       \n",
        "        boxes=prediction[0]['boxes'][prediction[0]['scores'] >score_threshold ]\n",
        "      else: \n",
        "        boxes=prediction[0]['boxes']\n",
        "    #print(boxes)\n",
        "    boxes = torch.mean(boxes, axis=0)\n",
        "     \n",
        "    # get crop img\n",
        "    im = Image.open(image)\n",
        "   \n",
        "    width, height = im.size\n",
        "    if math.isnan(boxes[0]) and math.isnan(boxes[1]) and math.isnan(boxes[2]) and math.isnan(boxes[3]):\n",
        "        im1 = im\n",
        "    else: \n",
        "        left = float(boxes[0].cpu().detach().numpy()) \n",
        "        top = float(boxes[1].cpu().detach().numpy())\n",
        "        right = float(boxes[2].cpu().detach().numpy())\n",
        "        bottom = float(boxes[3].cpu().detach().numpy())\n",
        "        im1 = im.crop((left, top, right, bottom))\n",
        "    resize = torchvision.transforms.Resize([100,100])\n",
        "    im1 = resize(im1)\n",
        "    # print(im1.size) #100\n",
        "    return im1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "d8a9aa23ef734a57930495cf60445c0e",
            "54007ad20fc1444c81abbab6a44b19c8",
            "a8475a6b034046ab81dd8711b7ffa2b5",
            "f8fe8302f7624848903f57a62e5eff04",
            "2e67cad59b634862a71b9df1e94d1125",
            "ac37249c06854e5da655e96471a3ed7c",
            "58f5ebae89e549969ff4e93dd4602599",
            "ec4905e8a3f54cf78e40b6be82a80ba0",
            "fe835d9b941444f5b81d3669ddc462be",
            "043792e5fa384988be1ab2d4f45ab491",
            "1cdb394699b140828ded03599cee7df6"
          ]
        },
        "id": "deGHjIZHMZlU",
        "outputId": "b2882a2a-12fa-4884-aaed-96c39ba3da01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8a9aa23ef734a57930495cf60445c0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing DeiTModel: ['distillation_classifier.weight', 'cls_classifier.bias', 'distillation_classifier.bias', 'cls_classifier.weight']\n",
            "- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['deit.pooler.dense.weight', 'deit.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DeiTFeatureExtractor, DeiTModel\n",
        "import torch\n",
        "\n",
        "\n",
        "img_trans_feature_extractor = DeiTFeatureExtractor.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
        "img_trans_model = DeiTModel.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxCdX_bNMbuX",
        "outputId": "645d192c-37f0-4d6d-e2d8-a3fdcab4a7f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')    #不同模型不同加载\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEPC42ttMeoB"
      },
      "outputs": [],
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(c_in, c_out)\n",
        "    \n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
        "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = number of incoming edges\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aArxLRctMhL6"
      },
      "outputs": [],
      "source": [
        "class HarmemeMemesDatasetAug2_cov(torch.utils.data.Dataset):\n",
        "    \"\"\"Uses jsonl data to preprocess and serve \n",
        "    dictionary of multimodal tensors for model input.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        img_dir,\n",
        "        split_flag=None,\n",
        "        balance=False,\n",
        "        dev_limit=None,\n",
        "        random_state=0,\n",
        "    ):\n",
        "\n",
        "        self.samples_frame = pd.read_json(\n",
        "            data_path, lines=True\n",
        "        )\n",
        "        self.samples_frame = self.samples_frame.reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "        self.samples_frame.image = self.samples_frame.apply(\n",
        "            lambda row: (img_dir + '/' + row.image), axis=1\n",
        "        )\n",
        "        if split_flag=='train':\n",
        "            self.ROI_samples = train_ROI_cov\n",
        "            self.ENT_samples = train_ENT_cov\n",
        "        elif split_flag=='val':\n",
        "            self.ROI_samples = val_ROI_cov\n",
        "            self.ENT_samples = val_ENT_cov\n",
        "        else:\n",
        "            self.ROI_samples = test_ROI_cov\n",
        "            self.ENT_samples = test_ENT_cov\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
        "        # img_file_name = self.samples_frame.loc[idx, \"image\"]\n",
        "        preprocess = Compose([\n",
        "                    Resize([100,100]),\n",
        "                    ToTensor()\n",
        "                    ])\n",
        "        image_FRCNN_input = img_feature_representation(self.samples_frame.loc[idx, \"image\"]) \n",
        "        FRCNNt_feature = fater_RCNN_process(fater_RCNN_model,image_FRCNN_input,self.samples_frame.loc[idx, \"image\"]) #image(224,224)\n",
        "        FRCNNt_feature = preprocess((FRCNNt_feature.convert('RGB')))\n",
        "        img_trams_inputs = img_trans_feature_extractor(FRCNNt_feature, return_tensors=\"pt\")\n",
        "        img_trams_outputs = img_trans_model(**img_trams_inputs)\n",
        "        img_trams_last_hidden_states = img_trams_outputs.last_hidden_state[:,1:,:] # [batch_size, tokens, hidden_dim]\n",
        "\n",
        "        cls_img = img_trams_outputs.last_hidden_state[:,0,:]\n",
        "        # img_trams_last_hidden_states = img_trams_outputs.pooler_output\n",
        "\n",
        "        # text\n",
        "        # text_info = text_feature_representation(self.samples_frame.loc[idx, \"text\"])\n",
        "        text_info = (self.samples_frame.loc[idx, \"text\"])\n",
        "        bert_input = bert_tokenizer(text_info, return_tensors='pt',truncation=True, max_length=50,padding=\"max_length\")\n",
        "        bert_feature = bert_model(**bert_input)\n",
        "        text_node_feats = bert_feature.last_hidden_state[:,1:,:]\n",
        "        cls_text = bert_feature.last_hidden_state[:,0,:]\n",
        "        # text_node_feats = bert_feature.pooler_output\n",
        "        #print(text_node_feats.shape)\n",
        "        '''\n",
        "        c_in = text_node_feats.shape[1] # 197\n",
        "        c_out = c_in\n",
        "        array = np.zeros((text_node_feats.shape[2])) \n",
        "        bias = torch.tensor(array)\n",
        "        array = np.identity((text_node_feats.shape[2]),dtype = np.double)\n",
        "        weight = torch.tensor(array)\n",
        "        array = np.ones((c_in, c_in))\n",
        "        tensor1 = torch.tensor(array)\n",
        "        adj_matrix = tensor1.unsqueeze(0)\n",
        "        layer = GCNLayer(c_in, c_out).cuda()\n",
        "        layer.projection.weight.data = weight.cuda().float()\n",
        "        layer.projection.bias.data = bias.cuda().float()\n",
        "       \n",
        "        with torch.no_grad():\n",
        "            text_gnn_out_feats = layer(text_node_feats.cuda().float(), adj_matrix.cuda().float())\n",
        "        '''\n",
        "        if \"labels\" in self.samples_frame.columns:\n",
        "\n",
        "            if self.samples_frame.loc[idx, \"labels\"][0]==\"not harmful\":\n",
        "                lab=0\n",
        "            else:\n",
        "                lab=1  \n",
        "   \n",
        "            label = torch.tensor(lab).to(device)  \n",
        "\n",
        "            \n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image_FRCNN_input\": image_FRCNN_input,\n",
        "                #\"FRCNNt_feature\": FRCNNt_feature,\n",
        "                'img_trams_last_hidden_states':img_trams_last_hidden_states,\n",
        "                'text_node_feats':text_node_feats,\n",
        "                'cls_img':cls_img,\n",
        "                'cls_text':cls_text,\n",
        "                #'gnn_out_feats':gnn_out_feats,\n",
        "                #'bert_feature':bert_feature,\n",
        "                #'text_gnn_out_feats':text_gnn_out_feats,\n",
        "                \"label\": label\n",
        "            }\n",
        "        \n",
        "\n",
        "        else:\n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image_FRCNN_input\": image_FRCNN_input,\n",
        "                #\"FRCNNt_feature\": FRCNNt_feature,\n",
        "                'img_trams_last_hidden_states':img_trams_last_hidden_states,\n",
        "                'text_node_feats':text_node_feats,\n",
        "                'cls_img':cls_img,\n",
        "                'cls_text':cls_text,\n",
        "                #'bert_feature':bert_feature,\n",
        "                #'gnn_out_feats':gnn_out_feats,\n",
        "                #'text_gnn_out_feats':text_gnn_out_feats,\n",
        "            }\n",
        "        #print(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjMsS3xvMkRB"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "hm_dataset_train_cov = HarmemeMemesDatasetAug2_cov(train_path_cov, data_dir_cov, split_flag='train')\n",
        "dataloader_train_cov = DataLoader(hm_dataset_train_cov, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "hm_dataset_val_cov = HarmemeMemesDatasetAug2_cov(dev_path_cov, data_dir_cov, split_flag='val')\n",
        "dataloader_val_cov = DataLoader(hm_dataset_val_cov, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=0)\n",
        "hm_dataset_test_cov = HarmemeMemesDatasetAug2_cov(test_path_cov, data_dir_cov, split_flag='test')\n",
        "dataloader_test_cov = DataLoader(hm_dataset_test_cov, batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxy6HoTiMnA6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3VN7e3sMpq4"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmRbwi1iy7sY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
        "        # x: Node feature matrix of shape [num_nodes, in_channels]\n",
        "        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qkO6RO4MrE1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    def __init__( self, input_dim, \\\n",
        "                        output_dim, \\\n",
        "                        support, \\\n",
        "                        act_func = None, \\\n",
        "                        featureless = False, \\\n",
        "                        dropout_rate = 0., \\\n",
        "                        bias=False):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.support = support\n",
        "        self.featureless = featureless\n",
        "        #self.linear = nn.Linear(input_dim,output_dim)\n",
        "        for i in range(len(self.support)):\n",
        "            setattr(self, 'W{}'.format(i), nn.Parameter(torch.randn(input_dim, output_dim)))\n",
        "\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(1, output_dim))\n",
        "\n",
        "        self.act_func = act_func\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(len(self.support)):\n",
        "            if self.featureless:\n",
        "                pre_sup = getattr(self, 'W{}'.format(i))\n",
        "            else:\n",
        "                pre_sup = x.mm(getattr(self, 'W{}'.format(i)))\n",
        "            \n",
        "            if i == 0:\n",
        "                out = self.support[i].mm(pre_sup)\n",
        "            else:\n",
        "                out += self.support[i].mm(pre_sup)\n",
        "\n",
        "        if self.act_func is not None:\n",
        "            out = self.act_func(out)\n",
        "\n",
        "        self.embedding = out\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDRGtVt0Ms_l"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimensionality of input features\n",
        "            c_out - Dimensionality of output features\n",
        "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The \n",
        "                        output features are equally split up over the heads if concat_heads=True.\n",
        "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
        "            alpha - Negative slope of the LeakyReLU activation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        if self.concat_heads:\n",
        "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
        "            c_out = c_out // num_heads\n",
        "        \n",
        "        # Sub-modules and parameters needed in the layer\n",
        "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
        "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
        "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
        "        \n",
        "        # Initialization from the original implementation\n",
        "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        \n",
        "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
        "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
        "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
        "        \n",
        "        # Apply linear layer and sort nodes by head\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
        "        \n",
        "        # We need to calculate the attention logits for every edge in the adjacency matrix \n",
        "        # Doing this on all possible combinations of nodes is very expensive\n",
        "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
        "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
        "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
        "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
        "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
        "        a_input = torch.cat([\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
        "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
        "        \n",
        "        # Calculate attention MLP output (independent for each head)\n",
        "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) \n",
        "        attn_logits = self.leakyrelu(attn_logits)\n",
        "        \n",
        "        # Map list of attention values back into a matrix\n",
        "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
        "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
        "        \n",
        "        # Weighted average of attention\n",
        "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
        "        if print_attn_probs:\n",
        "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
        "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "        \n",
        "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
        "        if self.concat_heads:\n",
        "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "        else:\n",
        "            node_feats = node_feats.mean(dim=2)\n",
        "        \n",
        "        return node_feats "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwD9UoTDMu04"
      },
      "outputs": [],
      "source": [
        "def build_edge_idx(num_nodes):\n",
        "    # Initialize edge index matrix\n",
        "    E = torch.zeros((2, num_nodes * (num_nodes - 1)), dtype=torch.long)\n",
        "    \n",
        "    # Populate 1st row\n",
        "    for node in range(num_nodes):\n",
        "        for neighbor in range(num_nodes - 1):\n",
        "            E[0, node * (num_nodes - 1) + neighbor] = node\n",
        "\n",
        "    # Populate 2nd row\n",
        "    neighbors = []\n",
        "    for node in range(num_nodes):\n",
        "        neighbors.append(list(np.arange(node)) + list(np.arange(node+1, num_nodes)))\n",
        "    E[1, :] = torch.Tensor([item for sublist in neighbors for item in sublist])\n",
        "    \n",
        "    return E\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ILug6B1Mxus"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Parameter\n",
        "import math\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=False):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(1, 1, out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.bmm(input, self.weight)\n",
        "        output = torch.bmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEPzCclDMzrT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the cross attention value features \n",
        "# Vanilla model\n",
        "import torch.nn.functional as F\n",
        "class CI_GNN(nn.Module):\n",
        "    def __init__(self,n_out):\n",
        "        \n",
        "        super(CI_GNN, self).__init__() \n",
        "        '''\n",
        "        self.fully_img_projection = nn.Linear(768, 768)\n",
        "        self.fully_text_projection = nn.Linear(768, 768)\n",
        "        self.elu = torch.nn.ELU()\n",
        "        self.GNN_att = GATLayer(768,768,num_heads=2)\n",
        "        self.pool = nn.MaxPool2d(3,stride = 2)\n",
        "        self.cls_projection = nn.Linear(768,383)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fully_connected_1 = nn.Linear(56684,4096)\n",
        "        self.fully_connected_2 = nn.Linear(4096,64)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out = nn.Linear(64,n_out)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        '''\n",
        "\n",
        "        self.GCN_img = GCN(768, 512, 256).to(device)\n",
        "        self.GCN_text = GCN(768, 512, 256).to(device)\n",
        "\n",
        "        self.fully_img_projection = nn.Linear(768, 512)\n",
        "        self.fully_text_projection = nn.Linear(768, 512)\n",
        "        self.fully_img_projection_2 = nn.Linear(512,512)\n",
        "        self.fully_text_projection_2 = nn.Linear(512,512)\n",
        "        self.fully_img_projection_3 = nn.Linear(256,128)\n",
        "        self.fully_text_projection_3 = nn.Linear(256,128)\n",
        "        self.elu = torch.nn.ELU()\n",
        "\n",
        "        self.gnn = GraphConvolution(768,768)\n",
        "        self.GNN_att_mul = GATLayer(128,64,num_heads=1)\n",
        "        self.GNN_att = GATLayer(64,64,num_heads=1)\n",
        "        self.pool = nn.MaxPool2d(3,stride = 2)\n",
        "        self.cls_projection = nn.Linear(768,31) #!!node_size \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fully_connected_1 = nn.Linear(3844,1024) \n",
        "        self.fully_connected_f = nn.Linear(1024,256)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.glue = nn.GELU()\n",
        "        self.out = nn.Linear(256,n_out)\n",
        "        self.batchnorm_1 = nn.BatchNorm1d(1024)\n",
        "        self.batchnorm_f = nn.BatchNorm1d(256)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "     \n",
        "    def fully_img_gnn(self,img,size):\n",
        "        #print('img.shape',img.shape) img.shape torch.Size([8, 197, 768])\n",
        "\n",
        "        c_in = img.shape[1] # 197\n",
        "        c_out = c_in\n",
        "        array = np.ones((size,c_in, c_in),dtype = np.float64)\n",
        "        adj_matrix = torch.tensor(array).float().to(device)\n",
        "        #print('adj_matrix',adj_matrix.shape) adj_matrix torch.Size([8, 198, 198])\n",
        "        node_feats = img\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.fully_img_projection(node_feats)   \n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours.to(device)\n",
        "        return node_feats\n",
        "\n",
        "    def fully_text_gnn(self,text,size):\n",
        "        #print('text.shape',text.shape) text.shape torch.Size([8, 100, 768])\n",
        "        c_in = text.shape[1] \n",
        "        c_out = c_in\n",
        "        '''\n",
        "        array = np.zeros((text.shape[2])) \n",
        "        bias = torch.tensor(array).to(device)\n",
        "        array = np.identity((text.shape[2]))\n",
        "        weight = torch.tensor(array).to(device)\n",
        "        '''\n",
        "        array = np.ones((size,c_in, c_in),dtype = np.float64)\n",
        "        adj_matrix = torch.tensor(array).float().to(device)\n",
        "        #print('adj_matrix',adj_matrix.shape) adj_matrix torch.Size([8, 100, 100])\n",
        "        node_feats = text\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.fully_img_projection(node_feats)   \n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours.to(device)\n",
        "        return node_feats\n",
        "\n",
        "    def multi_gnn(self,img,text,size):\n",
        "        node_feats = torch.concat((img,text), dim = 1) # torch.Size([8, 296, 768])\n",
        "        c_in = node_feats.shape[1]\n",
        "        array_1 = np.identity((c_in))\n",
        "        array_2 = np.ones((size,c_in, c_in))\n",
        "        array_3 = array_2-array_1\n",
        "  \n",
        "        tensor1 = torch.tensor(array_3)\n",
        "        adj_matrix = tensor1.float().to(device)\n",
        "\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        #node_feats = self.projection(node_feats)\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats\n",
        "  \n",
        "    def gnn_att(self, node,size):\n",
        "        c_in = node.shape[1] \n",
        "        c_out = c_in\n",
        "        array = np.ones((size,c_in, c_in),dtype = np.float64)\n",
        "        adj_matrix = torch.tensor(array).float().to(device)\n",
        "        \n",
        "        out = self.GNN_att(node,adj_matrix)\n",
        "        return out\n",
        "\n",
        "    def gnn_att_11(self, node,size):\n",
        "       \n",
        "        c_in = node.shape[1] \n",
        "        c_out = c_in\n",
        "        array = np.ones((size,c_in, c_in),dtype = np.float64)\n",
        "        adj_matrix = torch.tensor(array).float().to(device)\n",
        "        out = self.GNN_att_1(node,adj_matrix)\n",
        "        return out\n",
        "\n",
        "    def gnn_att_mul(self,img,text,size):\n",
        "        node_feats = torch.concat((img,text), dim = 1)\n",
        "        c_in = node_feats.shape[1] \n",
        "        c_out = c_in\n",
        "        array = np.ones((size,c_in, c_in),dtype = np.float64)\n",
        "        adj_matrix = torch.tensor(array).float().to(device)\n",
        "        out = self.GNN_att_mul(node_feats,adj_matrix)\n",
        "        return out\n",
        "\n",
        "    def feature_fusion(self,node,cls_img,cls_text):\n",
        "        cls = torch.concat((cls_img,cls_text),dim = 1)\n",
        "        node_feats = torch.concat((node,cls), dim = 1) # torch.Size([8, 148, 383])\n",
        "        s_g = self.sigmoid(node_feats) #torch.Size([8, 148, 383])\n",
        "        s_m = self.sigmoid(node_feats) #torch.Size([8, 148, 383])\n",
        "        # print(cls.shape) #torch.Size([8, 1, 383])\n",
        "        # print(node.shape) #torch.Size([8, 147, 383])\n",
        "        g = torch.bmm(node,s_g.permute(0,2,1)) # torch.Size([8, 147, 148])\n",
        "        c = torch.bmm(cls,s_m.permute(0,2,1)) # torch.Size([8, 1, 148])\n",
        "        f_soft = torch.concat((g,c),dim = 1)\n",
        "\n",
        "        alpha_g = self.sigmoid(node_feats)\n",
        "        alpha_m = self.sigmoid(node_feats)\n",
        "        s_g= torch.bernoulli(alpha_g)\n",
        "        s_m= torch.bernoulli(alpha_m)\n",
        "        f_hard = torch.concat((g,c),dim = 1)\n",
        "        return node_feats\n",
        "\n",
        "\n",
        "    def forward(self, img,text,cls_img,cls_text):\n",
        "            '''\n",
        "            if torch.Tensor.dim(img)!= 3:\n",
        "              img = img.unsqueeze(0)\n",
        "              text = text.unsqueeze(0)\n",
        "            size = img.shape[0]  \n",
        "            # print(img.shape) torch.Size([1, 197, 768])\n",
        "            # img_node = self.fully_img_gnn(img,size) #[8, 197, 768] \n",
        "            # text_node = self.fully_text_gnn(text,size) #[8, 99, 768]\n",
        "          \n",
        "            #print('1')\n",
        "            # print(img_node)\n",
        "            # print(text_node)\n",
        "            img_node = self.fully_img_projection_2(img_node) # (1576x512 and 768x512)\n",
        "            text_node = self.fully_text_projection_2(text_node)\n",
        "            # print('2')\n",
        "            # print(img_node)\n",
        "            # print(text_node)\n",
        "            img_node = self.elu(img_node)\n",
        "            text_node = self.elu(text_node)\n",
        "            # print('3')\n",
        "            # print(img_node)\n",
        "            # print(text_node)\n",
        "            node = self.multi_gnn(img_node,text_node,size)\n",
        "            # print('4')\n",
        "            # print(node)\n",
        "\n",
        "            node = self.gnn_att(node,size) #torch.Size([4, 246, 512])\n",
        "   \n",
        "            # print('5')\n",
        "            # print(node)\n",
        "            node = self.pool(node) #torch.Size([8, 147, 383]) =>  torch.Size([8, 147, 255]) =>torch.Size([4, 122, 255])\n",
        "      \n",
        "  \n",
        "            # print('6')\n",
        "            # print(node)\n",
        "\n",
        "            cls_img = self.cls_projection(cls_img) #torch.Size([8, 1, 383]) \n",
        "            cls_text = self.cls_projection(cls_text) \n",
        "\n",
        "            # print('7')\n",
        "            # print(cls)\n",
        "            fusion_feature = self.feature_fusion(node,cls_img,cls_text) #torch.Size([8, 148, 148])) # torch.Size([8, 148, 383]) => torch.Size([4, 124, 255])\n",
        "    \n",
        "            # print('8')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = torch.flatten(fusion_feature, 1) #torch.Size([8, 21904]) => torch.Size([8, 37995]) =>torch.Size([4, 31620])\n",
        " \n",
        "            # print('9')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.sigmoid(self.fully_connected_1(fusion_feature)) #8x37995  (4x31620 and 37995x16384)\n",
        "            # print('9.1')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.sigmoid(self.fully_connected_1_1(fusion_feature)) #8x37995 \n",
        "            # print('9.2')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.sigmoid(self.fully_connected_1_2(fusion_feature)) #8x37995 \n",
        "            # print('10')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.sigmoid(self.fully_connected_f(fusion_feature))\n",
        "            # print('11')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.sigmoid(fusion_feature)\n",
        "            # print('12')\n",
        "            # print(fusion_feature)\n",
        "    \n",
        "            y = self.softmax(self.out(fusion_feature)) # torch.Size([8, 2])\n",
        "            '''\n",
        "            \n",
        "            if torch.Tensor.dim(img)!= 3:\n",
        "              img = img.unsqueeze(0)\n",
        "              text = text.unsqueeze(0)\n",
        "            size = img.shape[0]  \n",
        "            # print(img.shape)# torch.Size([1, 197, 768])\n",
        "            # print(text.shape) torch.Size([1, 49, 768])\n",
        "            #print('1')\n",
        "            img_edge = build_edge_idx(img.shape[1]).to(device) #torch.Size([2, 38612])\n",
        "            text_edge = build_edge_idx(text.shape[1]).to(device)\n",
        "      \n",
        "            img_node = self.GCN_img(img, img_edge) #torch.Size([1, 197, 256])\n",
        "            text_node = self.GCN_text(text,text_edge)#torch.Size([1, 49, 256])\n",
        "    \n",
        "            img_node = self.fully_img_projection_3(img_node) # torch.Size([1, 197, 128])\n",
        "            text_node = self.fully_text_projection_3(text_node) # torch.Size([1, 49, 128])\n",
        "\n",
        "            img_node = self.elu(img_node)\n",
        "            text_node = self.elu(text_node)\n",
        "\n",
        "            node = self.gnn_att_mul(img_node,text_node,size) #torch.Size([1, 246, 64])\n",
        "\n",
        "            node = self.gnn_att(node,size) #torch.Size([1, 246, 64])\n",
        "\n",
        "            node = self.pool(node) #torch.Size([1, 122, 31])\n",
        "            \n",
        "            cls_img = self.cls_projection(cls_img) #torch.Size([8, 1, 383]) =>31\n",
        "            cls_text = self.cls_projection(cls_text) \n",
        "\n",
        "            fusion_feature = self.feature_fusion(node,cls_img,cls_text) #torch.Size([1, 124, 31])\n",
        "\n",
        "            fusion_feature = torch.flatten(fusion_feature, 1) #torch.Size([1, 3844])\n",
        "            fusion_feature = self.batchnorm_1(self.relu(self.fully_connected_1(fusion_feature)) )\n",
        "            fusion_feature = self.batchnorm_f(self.relu(self.fully_connected_f(fusion_feature)))\n",
        "            y = self.softmax(self.out(fusion_feature))\n",
        "           \n",
        "            '''\n",
        "            img_node = self.fully_img_gnn(img,size) # [8, 197, 768] => 256\n",
        "            text_node = self.fully_text_gnn(text,size) # [8, 99, 768]\n",
        "            \n",
        "            # print('1')\n",
        "            # print(img_node)\n",
        "            # print(text_node)\n",
        "            img_node = self.fully_img_projection_3(img_node) # (1576x512 and 768x512)\n",
        "            text_node = self.fully_text_projection_3(text_node)\n",
        "\n",
        "            # print('2')\n",
        "            # print(img_node)\n",
        "            # print(text_node)\n",
        "            img_node = self.elu(img_node)\n",
        "            text_node = self.elu(text_node)\n",
        "\n",
        "            # print('3')\n",
        "            # print(img_node)\n",
        "            # print(text_node)\n",
        "            node = self.gnn_att_mul(img_node,text_node,size) #(246x128 and 512x512)\n",
        "\n",
        "            # print('4')\n",
        "            # print(node)\n",
        "            # node = self.gnn_att(node,size) #torch.Size([4, 246, 512])\n",
        "   \n",
        "            # print('5')\n",
        "            # print(node)\n",
        "            node = self.pool(node) #torch.Size([8, 147, 383]) =>  torch.Size([8, 147, 255]) =>torch.Size([4, 122, 255])=>torch.Size([1, 122, 63])\n",
        "\n",
        "  \n",
        "            # print('6')\n",
        "            # print(node)\n",
        "            cls_img = self.cls_projection(cls_img) #torch.Size([8, 1, 383]) =>32\n",
        "            cls_text = self.cls_projection(cls_text) \n",
        "\n",
        "            # print('7')\n",
        "            # print(cls)\n",
        "            fusion_feature = self.feature_fusion(node,cls_img,cls_text) #torch.Size([8, 148, 148])) # torch.Size([8, 148, 383]) => torch.Size([4, 124, 255])\n",
        "    \n",
        "            # print('8')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = torch.flatten(fusion_feature, 1) #torch.Size([8, 21904]) => torch.Size([8, 37995]) =>torch.Size([4, 31620])\n",
        "  \n",
        "            # print('9')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.relu(self.fully_connected_1(fusion_feature)) #8x37995  (4x31620 and 37995x16384)\n",
        "          \n",
        "\n",
        "            # print('10')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.relu(self.fully_connected_f(fusion_feature))\n",
        "\n",
        "            # print('11')\n",
        "            # print(fusion_feature)\n",
        "            fusion_feature = self.relu(fusion_feature)\n",
        "            # print('12')\n",
        "            # print(fusion_feature)\n",
        "    \n",
        "            y = self.softmax(self.out(fusion_feature)) # torch.Size([8, 2])\n",
        "            '''\n",
        "            return (y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZTGQy8zM1mD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uhEcCNYM2_-"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c4ALr8qM4Mh"
      },
      "outputs": [],
      "source": [
        "def train_model(model,n_epochs, dataloader_train, dataloader_val):\n",
        "    epochs = n_epochs\n",
        "    train_acc_list=[]\n",
        "    val_acc_list=[]\n",
        "    train_loss_list=[]\n",
        "    val_loss_list=[]\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        total_loss_train = 0\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "        batch_count= 0\n",
        "        \n",
        "        for data in dataloader_train:\n",
        "            batch_count+=1\n",
        "            img = data['img_trams_last_hidden_states'].squeeze().to(device) #img.shape torch.Size([8, 198, 768])\n",
        "            text = data['text_node_feats'].squeeze().to(device)\n",
        "            cls_img = data['cls_img'].to(device) # torch.Size([8, 1, 768])\n",
        "            cls_text = data['cls_text'].to(device)\n",
        "            label_train = data['label'].to(device)\n",
        "            model.zero_grad()\n",
        "            try:\n",
        "                output = model(img,text,cls_img,cls_text)\n",
        "                \n",
        "                #print(output) # tensor([[0.5072, 0.4928]]\n",
        "                #print(label_train) # tensor([0], device='cuda:0')\n",
        "                loss = criterion(output, label_train)\n",
        "                #print(loss)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                \n",
        "                    # print(output.data) tensor([[0.5072, 0.4928]]\n",
        "                    _, predicted_train = torch.max(output.data, 1)\n",
        "                    # print(predicted_train) # tensor([0], device='cuda:0')\n",
        "                    total_train += label_train.size(0)\n",
        "                    correct_train += (predicted_train == label_train).sum().item()\n",
        "                    total_loss_train += loss.item()\n",
        "            except:\n",
        "                pass\n",
        "                # print(loss.item())\n",
        "            if batch_count % 10 ==0:\n",
        "                print(f'Epoch: {i+1}, batch_count : {batch_count}')\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "        train_loss = total_loss_train/total_train\n",
        "        \n",
        "        model.eval()\n",
        "        total_loss_val = 0\n",
        "        total_val = 0\n",
        "        correct_val = 0\n",
        "        with torch.no_grad():\n",
        "            for data in dataloader_val:      \n",
        "                img = data['img_trams_last_hidden_states'].squeeze().to(device) #img.shape torch.Size([8, 198, 768])\n",
        "                text = data['text_node_feats'].squeeze().to(device)\n",
        "                cls_img = data['cls_img'].to(device) # torch.Size([8, 1, 768])\n",
        "                cls_text = data['cls_text'].to(device)\n",
        "                label_val = data['label'].to(device)  \n",
        "                model.zero_grad()\n",
        "                output = model(img,text,cls_img,cls_text)\n",
        "\n",
        "                val_loss = criterion(output, label_val)\n",
        "                _, predicted_val = torch.max(output.data, 1)\n",
        "                total_val += label_val.size(0)\n",
        "                correct_val += (predicted_val == label_val).sum().item()                \n",
        "                total_loss_val += val_loss.item()  \n",
        "        print(\"Saving model...\") \n",
        "        path = '/content/drive/MyDrive/Sihiu/'\n",
        "        num = str(i)\n",
        "        torch.save(model.state_dict(), os.path.join(path,num+\"HARMP_CI-GNN(lr).pt\"))\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "        val_loss = total_loss_val/total_val\n",
        "\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "  \n",
        "    return  model, train_acc_list, val_acc_list, train_loss_list, val_loss_list, i               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMF7zz_JXMuR"
      },
      "source": [
        "## Lr= 0.00003 best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE7k1Q5mr5md",
        "outputId": "928ca1e2-b573-428a-ceea-3c5044be8db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, batch_count : 10\n",
            "Epoch: 1, batch_count : 20\n",
            "Epoch: 1, batch_count : 30\n",
            "Epoch: 1, batch_count : 40\n",
            "Epoch: 1, batch_count : 50\n",
            "Epoch: 1, batch_count : 60\n",
            "Epoch: 1, batch_count : 70\n",
            "Epoch: 1, batch_count : 80\n",
            "Epoch: 1, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 1: train_loss: 0.0213 train_acc: 58.7417 | val_loss: 0.0222 val_acc: 62.8169\n",
            "Epoch: 2, batch_count : 10\n",
            "Epoch: 2, batch_count : 20\n",
            "Epoch: 2, batch_count : 30\n",
            "Epoch: 2, batch_count : 40\n",
            "Epoch: 2, batch_count : 50\n",
            "Epoch: 2, batch_count : 60\n",
            "Epoch: 2, batch_count : 70\n",
            "Epoch: 2, batch_count : 80\n",
            "Epoch: 2, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 2: train_loss: 0.0195 train_acc: 66.5232 | val_loss: 0.0214 val_acc: 66.7606\n",
            "Epoch: 3, batch_count : 10\n",
            "Epoch: 3, batch_count : 20\n",
            "Epoch: 3, batch_count : 30\n",
            "Epoch: 3, batch_count : 40\n",
            "Epoch: 3, batch_count : 50\n",
            "Epoch: 3, batch_count : 60\n",
            "Epoch: 3, batch_count : 70\n",
            "Epoch: 3, batch_count : 80\n",
            "Epoch: 3, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 3: train_loss: 0.0175 train_acc: 74.3046 | val_loss: 0.0188 val_acc: 73.8028\n",
            "Epoch: 4, batch_count : 10\n",
            "Epoch: 4, batch_count : 20\n",
            "Epoch: 4, batch_count : 30\n",
            "Epoch: 4, batch_count : 40\n",
            "Epoch: 4, batch_count : 50\n",
            "Epoch: 4, batch_count : 60\n",
            "Epoch: 4, batch_count : 70\n",
            "Epoch: 4, batch_count : 80\n",
            "Epoch: 4, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 4: train_loss: 0.0154 train_acc: 81.7881 | val_loss: 0.0186 val_acc: 76.6197\n",
            "Epoch: 5, batch_count : 10\n",
            "Epoch: 5, batch_count : 20\n",
            "Epoch: 5, batch_count : 30\n",
            "Epoch: 5, batch_count : 40\n",
            "Epoch: 5, batch_count : 50\n",
            "Epoch: 5, batch_count : 60\n",
            "Epoch: 5, batch_count : 70\n",
            "Epoch: 5, batch_count : 80\n",
            "Epoch: 5, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 5: train_loss: 0.0139 train_acc: 87.5828 | val_loss: 0.0177 val_acc: 80.2817\n",
            "Epoch: 6, batch_count : 10\n",
            "Epoch: 6, batch_count : 20\n",
            "Epoch: 6, batch_count : 30\n",
            "Epoch: 6, batch_count : 40\n",
            "Epoch: 6, batch_count : 50\n",
            "Epoch: 6, batch_count : 60\n",
            "Epoch: 6, batch_count : 70\n",
            "Epoch: 6, batch_count : 80\n",
            "Epoch: 6, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 6: train_loss: 0.0129 train_acc: 90.3974 | val_loss: 0.0171 val_acc: 81.1268\n",
            "Epoch: 7, batch_count : 10\n",
            "Epoch: 7, batch_count : 20\n",
            "Epoch: 7, batch_count : 30\n",
            "Epoch: 7, batch_count : 40\n",
            "Epoch: 7, batch_count : 50\n",
            "Epoch: 7, batch_count : 60\n",
            "Epoch: 7, batch_count : 70\n",
            "Epoch: 7, batch_count : 80\n",
            "Epoch: 7, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 7: train_loss: 0.0122 train_acc: 92.9470 | val_loss: 0.0163 val_acc: 82.2535\n",
            "Epoch: 8, batch_count : 10\n",
            "Epoch: 8, batch_count : 20\n",
            "Epoch: 8, batch_count : 30\n",
            "Epoch: 8, batch_count : 40\n",
            "Epoch: 8, batch_count : 50\n",
            "Epoch: 8, batch_count : 60\n",
            "Epoch: 8, batch_count : 70\n",
            "Epoch: 8, batch_count : 80\n",
            "Epoch: 8, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 8: train_loss: 0.0120 train_acc: 93.6424 | val_loss: 0.0161 val_acc: 85.3521\n",
            "Epoch: 9, batch_count : 10\n",
            "Epoch: 9, batch_count : 20\n",
            "Epoch: 9, batch_count : 30\n",
            "Epoch: 9, batch_count : 40\n",
            "Epoch: 9, batch_count : 50\n",
            "Epoch: 9, batch_count : 60\n",
            "Epoch: 9, batch_count : 70\n",
            "Epoch: 9, batch_count : 80\n",
            "Epoch: 9, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 9: train_loss: 0.0119 train_acc: 93.7748 | val_loss: 0.0156 val_acc: 82.2535\n",
            "Epoch: 10, batch_count : 10\n",
            "Epoch: 10, batch_count : 20\n",
            "Epoch: 10, batch_count : 30\n",
            "Epoch: 10, batch_count : 40\n",
            "Epoch: 10, batch_count : 50\n",
            "Epoch: 10, batch_count : 60\n",
            "Epoch: 10, batch_count : 70\n",
            "Epoch: 10, batch_count : 80\n",
            "Epoch: 10, batch_count : 90\n",
            "Saving model...\n",
            "Epoch 10: train_loss: 0.0117 train_acc: 94.3709 | val_loss: 0.0158 val_acc: 86.1972\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr=0.00003\n",
        "output_size = 2\n",
        "model = CI_GNN(2)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "n_epochs = 10\n",
        "\n",
        "\n",
        "model_harm_c, train_acc_list_cov, val_acc_list_cov, train_loss_list_cov, val_loss_list_cov, epoc_num = train_model(model,\n",
        "                                                                                            n_epochs,dataloader_train_cov,\n",
        "                                                                                            dataloader_test_cov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "xnddU5S_sX4-",
        "outputId": "b4661a59-8ef9-424e-8088-d06e36e69cca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epoch = range(1,len(train_acc_list_cov)+1)\n",
        "plt.plot(epoch,train_acc_list_cov,color='blue', label='train')\n",
        "plt.plot(epoch,val_acc_list_cov,color='orange', label='test')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "spOKhbxSsbnq",
        "outputId": "58b765f5-ba22-4cac-98a0-760b77dfd553"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(epoch,train_loss_list_cov,color='blue', label='train')\n",
        "plt.plot(epoch,val_loss_list_cov,color='orange', label='test')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzdIutbJU_Ni"
      },
      "outputs": [],
      "source": [
        "def test_model(model, dataloader_test):\n",
        "    model.eval()\n",
        "    total_test = 0\n",
        "    correct_test =0\n",
        "    total_acc_test = 0\n",
        "    total_loss_test = 0\n",
        "    outputs = []\n",
        "    test_labels=[]\n",
        "    for data in dataloader_test:\n",
        "        img = data['img_trams_last_hidden_states'].squeeze().to(device) #img.shape torch.Size([8, 198, 768])\n",
        "        text = data['text_node_feats'].squeeze().to(device)\n",
        "        cls_img = data['cls_img'].to(device) # torch.Size([8, 1, 768])\n",
        "        cls_text = data['cls_text'].to(device)\n",
        "        label_test = data['label'].to(device)  \n",
        "\n",
        "        out = model(img,text,cls_img,cls_text)\n",
        "        outputs += list(out.cpu().data.numpy())\n",
        "        loss = criterion(out, label_test)\n",
        "        \n",
        "        _, predicted_test = torch.max(out.data, 1)\n",
        "        total_test += label_test.size(0)\n",
        "        correct_test += (predicted_test == label_test).sum().item()\n",
        "\n",
        "        total_loss_test += loss.item()\n",
        "        test_labels.append(label_test)\n",
        "    acc_test = 100 * correct_test / total_test\n",
        "    loss_test = total_loss_test/total_test   \n",
        "    \n",
        "    print(f'acc: {acc_test:.4f} loss: {loss_test:.4f}')\n",
        "    return outputs,test_labels          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsZMK_oQVCXF"
      },
      "outputs": [],
      "source": [
        "def model_evaluation(model, dataloader_test, model_name):\n",
        "    outputs,ls_truth = test_model(model, dataloader_test)\n",
        "    # Multiclass setting - Harmful\n",
        "    y_pred=[]\n",
        "    for i in outputs:\n",
        "    #     print(np.argmax(i))\n",
        "        y_pred.append(np.argmax(i))\n",
        "    # # np.argmax(outputs[:])\n",
        "    # outputs\n",
        "\n",
        "    # # Multiclass setting\n",
        "    aa = []\n",
        "    for batch in ls_truth:\n",
        "        aa+=batch\n",
        "\n",
        "    return y_pred, aa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZtIdq1qVFJ5",
        "outputId": "5faca575-b35b-4e81-beb6-b9ecf52256d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "acc: 86.1972 loss: 0.0158\n",
            "recall_score\t:  0.8624\n",
            "precision_score\t:  0.8622\n",
            "f1_score\t:  0.862\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86       182\n",
            "           1       0.84      0.88      0.86       173\n",
            "\n",
            "    accuracy                           0.86       355\n",
            "   macro avg       0.86      0.86      0.86       355\n",
            "weighted avg       0.86      0.86      0.86       355\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred,aa = model_evaluation(model, dataloader_test_cov,\"Stage 2-2\")\n",
        "ls = []\n",
        "for i in aa:\n",
        "    ls.append(i.item())\n",
        "rec = np.round(recall_score(ls, y_pred, average=\"macro\"),4)\n",
        "prec = np.round(precision_score(ls, y_pred, average=\"macro\"),4)\n",
        "f1 = np.round(f1_score(ls, y_pred, average=\"macro\"),4)\n",
        "# hl = np.round(hamming_loss(test_labels, y_pred),4)\n",
        "acc = np.round(accuracy_score(ls, y_pred),4)\n",
        "mae = np.round(mean_absolute_error(ls, y_pred),4)\n",
        "print(\"recall_score\\t: \",rec)\n",
        "print(\"precision_score\\t: \",prec)\n",
        "print(\"f1_score\\t: \",f1)\n",
        "# print(\"hamming_loss\\t: \",hl)\n",
        "# print(\"accuracy_score\\t: \",f1)\n",
        "print(classification_report(ls, y_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TS5CiXL1Pwyu",
        "ir3HfEWfL7Y6",
        "Cr1qRMe8MOSz",
        "y3VN7e3sMpq4",
        "g-D3i7uAXUA0",
        "0QEBs5sl0osR"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "043792e5fa384988be1ab2d4f45ab491": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cdb394699b140828ded03599cee7df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e67cad59b634862a71b9df1e94d1125": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54007ad20fc1444c81abbab6a44b19c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac37249c06854e5da655e96471a3ed7c",
            "placeholder": "​",
            "style": "IPY_MODEL_58f5ebae89e549969ff4e93dd4602599",
            "value": ""
          }
        },
        "58f5ebae89e549969ff4e93dd4602599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8475a6b034046ab81dd8711b7ffa2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec4905e8a3f54cf78e40b6be82a80ba0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe835d9b941444f5b81d3669ddc462be",
            "value": 0
          }
        },
        "ac37249c06854e5da655e96471a3ed7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a9aa23ef734a57930495cf60445c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54007ad20fc1444c81abbab6a44b19c8",
              "IPY_MODEL_a8475a6b034046ab81dd8711b7ffa2b5",
              "IPY_MODEL_f8fe8302f7624848903f57a62e5eff04"
            ],
            "layout": "IPY_MODEL_2e67cad59b634862a71b9df1e94d1125"
          }
        },
        "ec4905e8a3f54cf78e40b6be82a80ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f8fe8302f7624848903f57a62e5eff04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_043792e5fa384988be1ab2d4f45ab491",
            "placeholder": "​",
            "style": "IPY_MODEL_1cdb394699b140828ded03599cee7df6",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "fe835d9b941444f5b81d3669ddc462be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
